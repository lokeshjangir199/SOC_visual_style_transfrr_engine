Linear Regression
Logistic Regression
First we shall go into Machine Learning models for estimation of values. This is termed as regression. There are other models as well however linear regression is the simplest.

First we want you to go through the videos of first 2 weeks of this Coursera course

This is some additional theory to help/revise the concepts learnt.

Linear Regression
Linear Regression is one of the most fundamental models in Machine Learning. It assumes a linear relationship between target variable (y) and predictors (input variables) (x). Formally stating, (y) can be estimated assuming a linear combination of the input variables (x).

When we have a single predictor as the input, the model is called as Simple Linear Regression and when we have multiple predictors, the model is called Multiple Linear Regression.
The input variable (x) is a vector of the features of our dataset, for eg. when we are predicting housing prices from a dataset, the features of the dataset will be Area of the house, Locality, etc. and the output variable (y) in this case will be the housing price. The general representation of a Simple Linear Regression Model is -

                                            y = θ(0) + θ(1)*x
where θ(0) is known as the bias term, and θ in general is called as the weight vector, there are the parameters of the linear regression model. For multiple linear regression, the equation modifies to -

                                            y = transpose(Θ)*(X) 
where X is a vector containing all the features and Θ is a vector containing all the corresponding weights (bias and weights both) i.e. the parameters of the linear model. Also note that X here has an additonal feature - the constant term 1 which accounts for the intercept of the linear fit.

We define a function called the Cost Function that accounts for the prediction errors of the model. We try to minimize the cost function so that we can obtain a model that fits the data as good as possible. To reach the optima of the cost function, we employ a method that is used in almost all of machine learning called Gradient Descent.

Note
The relationship established between the target and the predictors is a statistical relation and not determinsitic. A deterministic relation is possible only when the data is actually prefectly linear.

Useful Resources
Overview of Gradient Descent

(Optional) This article's sections 2 and 3 explain the concept of Linear Regression as a Statistical Model, where you can calculate certain statistical quantities to determine and improve the accuracy of your model. This is known as Optimisation.

Logistic Regression
Logistic regression is a classifier that uses regression to obtain the probability of the input data belonging to one of various classes. Unlike Linear Regression, where the target values are continuous real valued, the target variables here are drawn from a finite set of discrete values. Formulation of logistic regression can be made where the parameters are estimated using gradient descent.

This article covers the major aspects of Logistic Regression and should give you a firm grasp on the mathematics behind this algorithm
Binary Logistic Regression
Binary logistic regression is used for classififcation into only 2 classes. Checkout this short read on Binary Logistic Regression.
The output of a binary logistic classifier is a probability value between 0 and 1, where 0 and 1 represent the two classes. By now you must be familiar with activation functions. The activation function used for Logistic Regression is the Sigmoid Function, which is also known as the Logistic Activation function. You can read more about it here.

Multinomial Logistic Regression
Logistic regression can be binary or multinomial in nature. The multinomial logistic regression is also termed as Softmax Regression.

Go through this article to go deeper into the concepts of Multinomial Logistic Regression.
The activation function used in the case of Multinomial Logistic Regression is known as the Softmax Activation Function.

Checkout this great article on Softmax basics.
Basically, the softmax function takes in a vector of real values as input, and generates a probability vector as the output. The dimension of this vector is the same as the number of classes for classification, and hence a single vector element having higher value than the rest of the vector elements helps us to classify the input vector (representative of an image, text, etc.) into a class.
